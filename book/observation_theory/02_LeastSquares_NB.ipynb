{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook exercise: fitting different models\n",
    "\n",
    "In this interactive notebook you will fit several models to a time series of height observations of a point on a glacier, to assess whether it is melting. \n",
    "\n",
    "Click {fa}`rocket` --> {guilabel}`Live Code` on the top right corner of this screen and then wait until all cells are executed.\n",
    "\n",
    "**Learning objectives:**\n",
    "- set-up an observation model\n",
    "- apply least-squares estimation\n",
    "- assess the estimation results considering redundancy, squared norm of residuals, under- and overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have ```m``` = 12 monthly measurements of the height of a point on a glacier. The measurements are obtained from a satellite laser altimeter.\n",
    "\n",
    "- Time [months]: ```t``` $=[0, 1, 2, \\ldots, 11]$\n",
    "- Observed heights [meters]: ```y``` $=[102.4, 98.2, 97.5, 97.9, 99.7, 100.7, 98.3, 94.2, 90.9, 86.1, 81.2, 76.9]$\n",
    "\n",
    "```t``` and ```y``` and ```m``` are already defined, so you can directly use these variables in your code.\n",
    "\n",
    "We will consider three different models, with the following observation equations:\n",
    "\n",
    "- Model 1: constant velocity, $\\mathbb{E}\\left( Y_i \\right) = x_0 + x_1 t_i$\n",
    "- Model 2: 8th order polynomial, $\\mathbb{E}\\left( Y_i \\right) =x_0 + x_1 t_i + x_2 t_i^2 +\\ldots+ x_8 t_i^8 = \\sum_{p=0}^8 x_p t_i^p $\n",
    "- Model 3: constant velocity + annual signal, $\\mathbb{E}\\left( Y_i \\right) = x_0 + x_1 t_i + x_2 \\cos \\Big(\\frac{2 \\pi t_i}{12} \\Big)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "thebe-remove-input-init"
    ]
   },
   "outputs": [],
   "source": [
    "import micropip\n",
    "await micropip.install(\"ipywidgets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "thebe-remove-input-init"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import operator\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def check_answer(variable_name, expected, comparison = operator.eq):\n",
    "    output = widgets.Output()\n",
    "    button = widgets.Button(description=\"Check answer\")\n",
    "    def _inner_check(button):\n",
    "        with output:\n",
    "            if comparison(globals()[variable_name], expected):\n",
    "                output.outputs = [{'name': 'stdout', 'text': 'Correct!',\n",
    "                                   'output_type': 'stream'}]\n",
    "            else:\n",
    "                output.outputs = [{'name': 'stdout', 'text': 'Incorrect!',\n",
    "                                   'output_type': 'stream'}]\n",
    "    button.on_click(_inner_check)\n",
    "    display(button, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "thebe-remove-input-init"
    ]
   },
   "outputs": [],
   "source": [
    "# times of observation [months]\n",
    "t = np.arange(12)\n",
    "\n",
    "# observed heights [m]\n",
    "y = [102.4, 98.2, 97.5, 97.9, 99.7, 100.7, 98.3, 94.2,\n",
    "                                     90.9, 86.1, 81.2, 76.9]\n",
    "\n",
    "m = len(t)\n",
    "\n",
    "A_1_ans = np.column_stack((np.ones(m), t))\n",
    "A_2_ans = np.column_stack((A_1_ans, t**2, t**3, t**4,\n",
    "                                    t**5, t**6, t**7, t**8))\n",
    "A_3_ans = np.column_stack((np.ones(m), t, np.cos(2*np.pi*t/12)))\n",
    "\n",
    "def lsqe_ans(y,A):\n",
    "    '''Apply least-squares estimation\n",
    "    Input:\n",
    "    y : vector with observations\n",
    "    A : design matrix \n",
    "    '''\n",
    "    # estimated parameters\n",
    "    xhat = np.linalg.inv(A.T @ A) @ A.T @ y\n",
    "    # adjusted observations\n",
    "    yhat = A @ xhat\n",
    "    # residuals\n",
    "    ehat = y - yhat\n",
    "    # squared norm of residuals\n",
    "    eTe = ehat.T @ ehat\n",
    "    \n",
    "    return xhat, yhat, ehat, eTe\n",
    "\n",
    "xhat_1_ans, yhat_1_ans, ehat_1_ans, eTe_1_ans = lsqe_ans(y,A_1_ans)\n",
    "xhat_2_ans, yhat_2_ans, ehat_2_ans, eTe_2_ans = lsqe_ans(y,A_2_ans)\n",
    "xhat_3_ans, yhat_3_ans, ehat_3_ans, eTe_3_ans = lsqe_ans(y,A_3_ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{card} Exercise 1\n",
    "\n",
    "With ```column_stack``` you can combine arrays (column vectors) to create a matrix. The design matrices for the first two models are already given. Add the matrix for the third model yourself, and check your answer below.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_1 = np.column_stack((np.ones(m), t))\n",
    "A_2 = np.column_stack((A_1, t**2, t**3, t**4, t**5, t**6, t**7, t**8))\n",
    "A_3 = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "thebe-remove-input-init"
    ]
   },
   "outputs": [],
   "source": [
    "check_answer(\"A_3\",A_3_ans, np.array_equiv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{card}  Exercise 2\n",
    "\n",
    "You can use the NumPy function ```np.linalg.inv``` to compute the inverse of a matrix. Recall that for a matrix product $\\mathrm{A}\\cdot \\mathrm{B} $ you can use ```A @ B```. Complete the function below. The transpose $\\mathrm{A}^T$ is obtained with ```A.T```.\n",
    "\n",
    "First complete the function ```lsqe```, and check whether it is correct below.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lsqe(y,A):\n",
    "    '''Apply least-squares estimation\n",
    "    Input:\n",
    "    y : vector with observations\n",
    "    A : design matrix \n",
    "    '''\n",
    "    # estimated parameters\n",
    "    xhat = ?\n",
    "    # adjusted observations\n",
    "    yhat = A @ xhat\n",
    "    # residuals\n",
    "    ehat = ?\n",
    "    # squared norm of residuals\n",
    "    eTe = ehat.T @ ehat\n",
    "    \n",
    "    return xhat, yhat, ehat, eTe\n",
    "\n",
    "xhat_1, yhat_1, ehat_1, eTe_1 = lsqe(y,A_1)\n",
    "xhat_2, yhat_2, ehat_2, eTe_2 = lsqe(y,A_2)\n",
    "xhat_3, yhat_3, ehat_3, eTe_3 = lsqe(y,A_3)\n",
    "\n",
    "print(f'Redundancy of linear trend model: '\n",
    "      f'{m - np.shape(A_1)[1]}')\n",
    "print(f'Redundancy of 8th order polynomial model: '\n",
    "      f'{m - np.shape(A_2)[1]}')\n",
    "print(f'Redundancy of linear trend + annual signal model: '\n",
    "      f'{m - np.shape(A_3)[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check whether ```xhat_1``` is correct (if not, you need to correct your ```lsqe``` function).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "thebe-remove-input-init"
    ]
   },
   "outputs": [],
   "source": [
    "check_answer(\"xhat_1\", xhat_1_ans, np.array_equiv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check whether ```eTe_1``` is correct (if not, you need to correct your ```lsqe``` function).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "thebe-remove-input-init"
    ]
   },
   "outputs": [],
   "source": [
    "check_answer(\"eTe_1\", eTe_1_ans, np.array_equiv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ```{admonition} Solution\n",
    ":class: tip, dropdown\n",
    "In case your code did not work, here's the correct solution:\n",
    "\n",
    "<code>xhat = np.linalg.inv(A.T @ A) @ A.T @ y</code>\n",
    "\n",
    "<code>ehat = y-yhat</code>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Running the next cell will plot the observations and the three fitted models in one figure, and the residuals for each model in another figure.** Note that the code with the function <code>plot_results</code> is hidden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "thebe-remove-input-init"
    ]
   },
   "outputs": [],
   "source": [
    "def plot_results():\n",
    "    print(f'Squared norm of residuals model 1: {eTe_1_ans:.3f}')\n",
    "    print(f'Squared norm of residuals model 2: {eTe_2_ans:.3f}')\n",
    "    print(f'Squared norm of residuals model 3: {eTe_3_ans:.3f}')\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize = (16, 6))\n",
    "    # left side plot : observations and fitted models\n",
    "    ax[0].plot(t, y, 'kx', label='observations')\n",
    "    ax[0].plot(t, yhat_1_ans, color='r', label='model 1')\n",
    "    ax[0].plot(t, yhat_2_ans, color='c', label='model 2')\n",
    "    ax[0].plot(t, yhat_3_ans, color='b', label='model 3')\n",
    "    ax[0].set_ylabel('height [meters]')\n",
    "    ax[0].set_xlabel('time [months]')\n",
    "    ax[0].set_title('Observations and fitted models')\n",
    "    ax[0].set_xlim(-0.2, (m-1)+0.2)\n",
    "    ax[0].legend()\n",
    "    \n",
    "    # right side plot : residuals\n",
    "    ax[1].plot(t, ehat_1_ans, '+r', label='model 1')\n",
    "    ax[1].plot(t, ehat_2_ans, '+c', label='model 2')\n",
    "    ax[1].plot(t, ehat_3_ans, '+b', label='model 3')\n",
    "    ax[1].set_ylabel('height [meters]')\n",
    "    ax[1].set_xlabel('time [months]')\n",
    "    ax[1].set_title('Residuals')\n",
    "    ax[1].set_xlim(-0.2, 11.2)\n",
    "    ax[1].legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{card}  Wrap-up exercises\n",
    "\n",
    "\n",
    "One of the models is *underfitting*. Try to reason what is meant by this and which model this applies to.\n",
    "\n",
    " ```{admonition} Solution\n",
    ":class: tip, dropdown\n",
    "Model 1 obviously does not capture the annual signal, so it is does not accurately describe the relation between observations and height change. This is referred to as underfitting.\n",
    "```\n",
    "       \n",
    "One of the models is *overfitting*. Try to reason what is meant by this and which model this applies to.  \n",
    "\n",
    " ```{admonition} Solution\n",
    ":class: tip, dropdown\n",
    "Model 2 is overfitting - it results in very small residuals since we include a lot of parameters, but what is the physical interpretation? Including higher orders will result in an even better fit (ultimately the perfect fit, see below). A big problem is that most likely this model will not be able to predict the height change for the future. This is illustrated in the figure below, where we predict the height for the coming 2 months, and see how the polynomial model predicts a very unrealistic increase in height.\n",
    "```\n",
    "\n",
    "What would happen if you would fit a 11th order polynomial?\n",
    "\n",
    " ```{admonition} Solution\n",
    ":class: tip, dropdown\n",
    "This would result in a perfect fit: $\\mathrm{A}$ becomes a full-rank square matrix, such that $\\hat{\\mathrm{x}} = \\mathrm{A}^{-1} \\mathrm{y}$ and the residuals become 0.\n",
    "```\n",
    "\n",
    "Do the residuals (see plot) behave as expected for each of the models.\n",
    "\n",
    " ```{admonition} Solution\n",
    ":class: tip, dropdown\n",
    "No, you would expect them to fluctuate randomly around zero, but especially for model 1 you can see a clear residual signal, which is an indication that the model is too simplistic.  \n",
    "```\n",
    "    \n",
    "Discuss why the squared norm of residuals is quite different for the 3 models.\n",
    "\n",
    " ```{admonition} Solution\n",
    ":class: tip, dropdown\n",
    "The squared norm for model 1 is very large, due to the annual signal that is not accounted for. The squared norm for model 2 is very small, so it results in a very good fit (due to the high order of the polynomial). Model 3 results in a somewhat larger squared norm than model 2 - this is due to the additional parameters in model 2, which will result in a better fit (i.e., it allows to better capture the actual height variation over time).\n",
    "```\n",
    "\n",
    "How could we quantitatvely assess whether model 3 is a good choice (this is a topic we will discuss later, but try to think for yourself how it could be done).\n",
    "\n",
    " ```{admonition} Solution\n",
    ":class: tip, dropdown\n",
    "You would somehow have to compare the size of the residuals with the precision as described by $\\Sigma_{\\epsilon}$.\n",
    "```\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "thebe-remove-input-init"
    ]
   },
   "outputs": [],
   "source": [
    "def plot_prediction():\n",
    "    # create array with times\n",
    "    t_pred = np.arange(14)\n",
    "    m_pred = len(t_pred)\n",
    "\n",
    "    # create the A-matrices for the given times\n",
    "    A_pred_1 = np.column_stack((np.ones(m_pred), t_pred))\n",
    "    A_pred_2 = np.column_stack((A_pred_1, t_pred**2, t_pred**3, t_pred**4, t_pred**5, t_pred**6, t_pred**7, t_pred**8))\n",
    "    A_pred_3 = np.column_stack((np.ones(m_pred), t_pred, np.cos(2*np.pi*t_pred/12)))\n",
    "\n",
    "    # calculate predicted heights\n",
    "    model_1 = A_pred_1 @ xhat_1\n",
    "    model_2 = A_pred_2 @ xhat_2\n",
    "    model_3 = A_pred_3 @ xhat_3\n",
    "\n",
    "    # plot the predicted heights bases on the three models\n",
    "    plt.figure()\n",
    "    plt.plot(t_pred, model_1, color='r', label='model 1')\n",
    "    plt.plot(t_pred, model_2, color='c', label='model 2')\n",
    "    plt.plot(t_pred, model_3, color='b', label='model 3')\n",
    "    plt.xlim(-0.2, (m_pred-1)+0.2)\n",
    "    plt.xlabel('time [months]')\n",
    "    plt.ylabel('predicted height [meters]')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Running the next cell will plot the fitted models including predictions for the next two months.** Note that the code with the function <code>plot_prediction</code> is hidden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prediction()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
